{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "This notebook prepares the images for a new class called \"unknown\". This class will be build using images from the [iFood-2019 dataset](https://www.kaggle.com/competitions/ifood-2019-fgvc6/data) dataset that contains 251 food types. This dataset can be considered as an extension of our target Food-101 dataset, so some of these 251 types already exist in the Food-101 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Unknown Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries and creating taget directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import shutil\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "from pathlib import Path\n",
    "from modules.dataloaders import create_dataloaders\n",
    "from bing_image_downloader import downloader\n",
    "\n",
    "# Define some constants\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "BATCH_SIZE = 64\n",
    "AMOUNT_TO_GET = 1.0\n",
    "SEED = 42\n",
    "\n",
    "# Define target data directory\n",
    "target_dir_food101_name = f\"../data/food-101_{str(int(AMOUNT_TO_GET*100))}_percent\"\n",
    "target_dir_food101_name_unknown = f\"../data/food-101_{str(int(AMOUNT_TO_GET*100))}_percent_unknown_2\"\n",
    "\n",
    "# Setup training and test directories\n",
    "target_dir_food101 = Path(target_dir_food101_name)\n",
    "train_dir_food101 = target_dir_food101 / \"train\"\n",
    "test_dir_food101 = target_dir_food101 / \"test\"\n",
    "\n",
    "# Create unknown directores\n",
    "target_dir_food101_unknown = Path(target_dir_food101_name_unknown)\n",
    "train_dir_food101_unknown = target_dir_food101_unknown / \"train\" / \"unknown\"\n",
    "test_dir_food101_unknown = target_dir_food101_unknown / \"test\" / \"unknown\"\n",
    "target_dir_food101_unknown.mkdir(parents=True, exist_ok=True)\n",
    "train_dir_food101_unknown.mkdir(parents=True, exist_ok=True)\n",
    "test_dir_food101_unknown.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create target model directory\n",
    "model_dir = Path(\"../models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the class names for the Food101 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image size\n",
    "IMG_SIZE = 384\n",
    "\n",
    "# Manual transforms for the training dataset\n",
    "manual_transforms = v2.Compose([           \n",
    "    v2.RandomCrop((IMG_SIZE, IMG_SIZE)),    \n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),    \n",
    "])\n",
    "\n",
    "# ViT-Base transforms\n",
    "# Manual transforms for the training dataset\n",
    "manual_transforms_aug_norm_train_vitb = v2.Compose([    \n",
    "    v2.TrivialAugmentWide(),\n",
    "    v2.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    v2.CenterCrop((IMG_SIZE, IMG_SIZE)),    \n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "# Manual transforms for the test dataset\n",
    "manual_transforms_aug_norm_test_vitb = v2.Compose([    \n",
    "    v2.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    v2.CenterCrop((IMG_SIZE, IMG_SIZE)),    \n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "# Get the class names for the Food101 dataset\n",
    "_, _, classes_food101_list = create_dataloaders(\n",
    "    train_dir=train_dir_food101,\n",
    "    test_dir=test_dir_food101,\n",
    "    train_transform=manual_transforms_aug_norm_train_vitb,\n",
    "    test_transform=manual_transforms_aug_norm_test_vitb,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 101)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples_train = [len(list(Path(train_dir_food101).glob(f\"**/{classes}/*.jpg\"))) for classes in classes_food101_list]\n",
    "n_samples_test = [len(list(Path(test_dir_food101).glob(f\"**/{classes}/*.jpg\"))) for classes in classes_food101_list]\n",
    "#test_image_path_list = list(Path(train_dir).glob(\"**/apple_pie/*.jpg\")) # get list all image paths from test data \n",
    "#test_image_path_list\n",
    "len(n_samples_train), len(n_samples_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the ifood-2019 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'macaron', 1: 'beignet', 2: 'cruller', 3: 'cockle_food', 4: 'samosa', 5: 'tiramisu', 6: 'tostada', 7: 'moussaka', 8: 'dumpling', 9: 'sashimi', 10: 'knish', 11: 'croquette', 12: 'couscous', 13: 'porridge', 14: 'stuffed_cabbage', 15: 'seaweed_salad', 16: 'chow_mein', 17: 'rigatoni', 18: 'beef_tartare', 19: 'cannoli', 20: 'foie_gras', 21: 'cupcake', 22: 'osso_buco', 23: 'pad_thai', 24: 'poutine', 25: 'ramen', 26: 'pulled_pork_sandwich', 27: 'bibimbap', 28: 'chicken_kiev', 29: 'apple_pie', 30: 'risotto', 31: 'fruitcake', 32: 'chop_suey', 33: 'haggis', 34: 'scrambled_eggs', 35: 'frittata', 36: 'scampi', 37: 'sushi', 38: 'orzo', 39: 'fritter', 40: 'nacho', 41: 'beef_stroganoff', 42: 'beef_wellington', 43: 'spring_roll', 44: 'savarin', 45: 'crayfish_food', 46: 'souffle', 47: 'adobo', 48: 'streusel', 49: 'deviled_egg', 50: 'escargot', 51: 'club_sandwich', 52: 'carrot_cake', 53: 'falafel', 54: 'farfalle', 55: 'terrine', 56: 'poached_egg', 57: 'gnocchi', 58: 'bubble_and_squeak', 59: 'egg_roll', 60: 'caprese_salad', 61: 'sauerkraut', 62: 'creme_brulee', 63: 'pavlova', 64: 'fondue', 65: 'scallop', 66: 'jambalaya', 67: 'tempura', 68: 'chocolate_cake', 69: 'potpie', 70: 'spaghetti_bolognese', 71: 'sukiyaki', 72: 'applesauce', 73: 'baklava', 74: 'salisbury_steak', 75: 'linguine', 76: 'edamame', 77: 'coq_au_vin', 78: 'tamale', 79: 'macaroni_and_cheese', 80: 'kedgeree', 81: 'garlic_bread', 82: 'beet_salad', 83: 'steak_tartare', 84: 'vermicelli', 85: 'pate', 86: 'pancake', 87: 'tetrazzini', 88: 'onion_rings', 89: 'red_velvet_cake', 90: 'compote', 91: 'lobster_food', 92: 'chicken_curry', 93: 'chicken_wing', 94: 'caesar_salad', 95: 'succotash', 96: 'hummus', 97: 'fish_and_chips', 98: 'lasagna', 99: 'lutefisk', 100: 'sloppy_joe', 101: 'gingerbread', 102: 'crab_cake', 103: 'sauerbraten', 104: 'peking_duck', 105: 'guacamole', 106: 'ham_sandwich', 107: 'crumpet', 108: 'taco', 109: 'strawberry_shortcake', 110: 'clam_chowder', 111: 'cottage_pie', 112: 'croque_madame', 113: 'french_onion_soup', 114: 'beef_carpaccio', 115: 'torte', 116: 'poi', 117: 'crab_food', 118: 'bacon_and_eggs', 119: 'coffee_cake', 120: 'custard', 121: 'syllabub', 122: 'pork_chop', 123: 'fried_rice', 124: 'boiled_egg', 125: 'galantine', 126: 'brisket', 127: 'reuben', 128: 'schnitzel', 129: 'ambrosia_food', 130: 'gyoza', 131: 'jerky', 132: 'ravioli', 133: 'fried_calamari', 134: 'spaghetti_carbonara', 135: 'miso_soup', 136: 'frozen_yogurt', 137: 'wonton', 138: 'panna_cotta', 139: 'french_toast', 140: 'enchilada', 141: 'ceviche', 142: 'fettuccine', 143: 'chili', 144: 'flan', 145: 'kabob', 146: 'sponge_cake', 147: 'casserole', 148: 'paella', 149: 'blancmange', 150: 'bruschetta', 151: 'tortellini', 152: 'grilled_salmon', 153: 'french_fries', 154: 'shrimp_and_grits', 155: 'churro', 156: 'donut', 157: 'meat_loaf_food', 158: 'meatball', 159: 'scrapple', 160: 'strudel', 161: 'coconut_cake', 162: 'marble_cake', 163: 'filet_mignon', 164: 'hamburger', 165: 'fried_egg', 166: 'tuna_tartare', 167: 'penne', 168: 'eggs_benedict', 169: 'bread_pudding', 170: 'takoyaki', 171: 'tenderloin', 172: 'chocolate_mousse', 173: 'baked_alaska', 174: 'hot_dog', 175: 'confit', 176: 'ham_and_eggs', 177: 'biryani', 178: 'greek_salad', 179: 'huevos_rancheros', 180: 'tagliatelle', 181: 'stuffed_peppers', 182: 'cannelloni', 183: 'pizza', 184: 'sausage_roll', 185: 'chicken_quesadilla', 186: 'hot_and_sour_soup', 187: 'prime_rib', 188: 'cheesecake', 189: 'limpet_food', 190: 'ziti', 191: 'mussel', 192: 'manicotti', 193: 'ice_cream', 194: 'waffle', 195: 'oyster', 196: 'omelette', 197: 'clam_food', 198: 'burrito', 199: 'roulade', 200: 'lobster_bisque', 201: 'grilled_cheese_sandwich', 202: 'gyro', 203: 'pound_cake', 204: 'pho', 205: 'lobster_roll_sandwich', 206: 'baby_back_rib', 207: 'tapenade', 208: 'pepper_steak', 209: 'welsh_rarebit', 210: 'pilaf', 211: 'dolmas', 212: 'coquilles_saint_jacques', 213: 'veal_cordon_bleu', 214: 'shirred_egg', 215: 'barbecued_wing', 216: 'lobster_thermidor', 217: 'steak_au_poivre', 218: 'huitre', 219: 'chiffon_cake', 220: 'profiterole', 221: 'toad_in_the_hole', 222: 'chicken_marengo', 223: 'victoria_sandwich', 224: 'tamale_pie', 225: 'boston_cream_pie', 226: 'fish_stick', 227: 'crumb_cake', 228: 'chicken_provencale', 229: 'vol_au_vent', 230: 'entrecote', 231: 'carbonnade_flamande', 232: 'bacon_lettuce_tomato_sandwich', 233: 'scotch_egg', 234: 'pirogi', 235: 'peach_melba', 236: 'upside_down_cake', 237: 'applesauce_cake', 238: 'rugulah', 239: 'rock_cake', 240: 'barbecued_spareribs', 241: 'beef_bourguignonne', 242: 'rissole', 243: 'mostaccioli', 244: 'apple_turnover', 245: 'matzo_ball', 246: 'chicken_cordon_bleu', 247: 'eccles_cake', 248: 'moo_goo_gai_pan', 249: 'buffalo_wing', 250: 'stuffed_tomato'}\n"
     ]
    }
   ],
   "source": [
    "target_dir_food251_name = \"../data/ifood-2019-fgvc6\"\n",
    "\n",
    "# Setup training and test directories\n",
    "target_dir_food251 = Path(target_dir_food251_name)\n",
    "train_dir_food251 = target_dir_food251 / \"train_set\"\n",
    "val_dir_food251 = target_dir_food251 / \"val_set\"\n",
    "\n",
    "# Path to the file\n",
    "class_file_path = target_dir_food251 / \"class_list.txt\"\n",
    "train_labels_path = target_dir_food251 / \"train_labels.csv\"\n",
    "val_labels_path = target_dir_food251 / \"val_labels.csv\"\n",
    "\n",
    "# Initialize an empty list to store class names\n",
    "classes_food251_dict = {}\n",
    "\n",
    "# Open and read the file\n",
    "with open(class_file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Split the line into components\n",
    "        parts = line.strip().split(\" \", 1)\n",
    "        if len(parts) > 1:\n",
    "            classes_food251_dict.update({int(parts[0]): parts[1]})            \n",
    "\n",
    "# Print the result\n",
    "print(classes_food251_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying the remaining classes—those in iFood-2019 that do not belong to Food-101—also involves cleaning the class names, such as through lemmatization, to achieve a good match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatization\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "classes_food101_lem_list = [lemmatizer.lemmatize(word) for word in classes_food101_list]\n",
    "classes_food251_lem_dict = {key: lemmatizer.lemmatize(word) for key, word in classes_food251_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look the classes in food101 that still end with \"s\" and replace them\n",
    "classes_food101_lem_list = [c[:-1] if c.endswith(\"s\") else c for c in classes_food101_list]\n",
    "\n",
    "# And check it out\n",
    "for c in classes_food101_lem_list:\n",
    "    if c.endswith(\"s\"):\n",
    "        print(c)\n",
    "\n",
    "classes_food101_lem_list = [c.replace(\"_\", \"\") for c in classes_food101_lem_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look the classes in food251 that still end with \"s\" and replace them\n",
    "classes_food251_lem_dict = {key: c[:-1] if c.endswith(\"s\") else c for key, c in classes_food251_dict.items()}\n",
    "\n",
    "# And check it out\n",
    "for c in classes_food251_lem_dict.values():\n",
    "    if c.endswith(\"s\"):\n",
    "        print(c)\n",
    "classes_food251_lem_dict = {key: c.replace(\"_\", \"\") for key, c in classes_food251_lem_dict.items()}\n",
    "\n",
    "# Create a new dictionary excluding related classes\n",
    "classes_food251_lem_dict = {key: val for key, val in classes_food251_lem_dict.items() if val != 'entrecote'}\n",
    "classes_food251_lem_dict = {key: val for key, val in classes_food251_lem_dict.items() if val != 'tenderloin'}\n",
    "classes_food251_lem_dict = {key: val for key, val in classes_food251_lem_dict.items() if val != 'brisket'}\n",
    "classes_food251_lem_dict = {key: val for key, val in classes_food251_lem_dict.items() if val != 'biryani'}\n",
    "classes_food251_lem_dict = {key: val for key, val in classes_food251_lem_dict.items() if val != 'meatball'}\n",
    "classes_food251_lem_dict = {key: val for key, val in classes_food251_lem_dict.items() if val != 'reuben'}\n",
    "classes_food251_lem_dict = {key: val for key, val in classes_food251_lem_dict.items() if val != 'schnitzel'}\n",
    "classes_food251_lem_dict = {key: val for key, val in classes_food251_lem_dict.items() if val != 'sukiyaki'}\n",
    "classes_food251_lem_dict = {key: val for key, val in classes_food251_lem_dict.items() if val != 'chowmein'}\n",
    "classes_food251_lem_dict = {key: val for key, val in classes_food251_lem_dict.items() if not('steak' in val)}\n",
    "classes_food251_lem_dict = {key: val for key, val in classes_food251_lem_dict.items() if not('sandwich' in val)}\n",
    "classes_food251_lem_dict = {key: val for key, val in classes_food251_lem_dict.items() if not('salad' in val)}\n",
    "classes_food251_lem_dict = {key: val for key, val in classes_food251_lem_dict.items() if not('rib' in val)}\n",
    "classes_food251_lem_dict = {key: val for key, val in classes_food251_lem_dict.items() if not('chicken' in val)}\n",
    "classes_food251_lem_dict = {key: val for key, val in classes_food251_lem_dict.items() if not('beef' in val)}\n",
    "\n",
    "classes_food251_lem_dict = {\n",
    "    key: val \n",
    "    for key, val in classes_food251_lem_dict.items() \n",
    "    if not any(val in food for food in classes_food101_lem_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify how many images per remaining class should contain the new unknown class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of remaining classes: 128\n",
      "Number of samples per remaining class for training: 5\n",
      "Number of samples per remaining class for training: 1\n"
     ]
    }
   ],
   "source": [
    "# Specify how many images per remaining class should contain the new unknown class\n",
    "remaining_classes = set(classes_food251_lem_dict.values()) - set(classes_food101_lem_list)\n",
    "remaining_classes_dict = {key: value for key, value in classes_food251_lem_dict.items() if value in remaining_classes}\n",
    "n_samples_per_remaining_class_train = int(np.mean(n_samples_train) / len(remaining_classes))\n",
    "n_samples_per_remaining_class_test = int(np.mean(n_samples_test) / len(remaining_classes))\n",
    "print(f\"Number of remaining classes: {len(remaining_classes_dict)}\")\n",
    "print(f\"Number of samples per remaining class for training: {n_samples_per_remaining_class_train}\")\n",
    "print(f\"Number of samples per remaining class for training: {n_samples_per_remaining_class_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data frame that contains the image name, label, and class name of the remaning image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of the reamining classes  \n",
    "df_remaining_classes = pd.DataFrame(remaining_classes_dict.items(), columns=['label', 'class'])\n",
    "df_train_labels = pd.read_csv(train_labels_path)\n",
    "df_val_labels = pd.read_csv(val_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend the dataframe with labels\n",
    "df_remaining_train_labels = df_train_labels.merge(df_remaining_classes, how='right', on='label')\n",
    "df_remaining_val_labels = df_val_labels.merge(df_remaining_classes, how='right', on='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>label</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_062355.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>cruller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_062356.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>cruller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_062357.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>cruller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_062358.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>cruller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_062359.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>cruller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train_062360.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>cruller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train_062361.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>cruller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train_062362.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>cruller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train_062363.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>cruller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train_062364.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>cruller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           img_name  label    class\n",
       "0  train_062355.jpg      2  cruller\n",
       "1  train_062356.jpg      2  cruller\n",
       "2  train_062357.jpg      2  cruller\n",
       "3  train_062358.jpg      2  cruller\n",
       "4  train_062359.jpg      2  cruller\n",
       "5  train_062360.jpg      2  cruller\n",
       "6  train_062361.jpg      2  cruller\n",
       "7  train_062362.jpg      2  cruller\n",
       "8  train_062363.jpg      2  cruller\n",
       "9  train_062364.jpg      2  cruller"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore the result\n",
    "df_remaining_train_labels.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copies a random selection of image files from the source directory to the destination directory based on labels provided in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_random_samples(df, source_dir, destination_dir, n_samples_per_class, seed):\n",
    "    \"\"\"\n",
    "    Copy random samples from the source directory to the destination directory.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing the labels and image names.\n",
    "        source_dir (str): Path to the source directory.\n",
    "        destination_dir (str): Path to the destination directory.\n",
    "        n_samples_per_class (int): Number of samples to copy per class.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Ensure the destination directory exists\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "    # Loop over each label\n",
    "    for _, group in df.groupby('label'):\n",
    "        # Sample the group and take the image names \n",
    "        selected_files = group.sample(n=n_samples_per_class, random_state=seed)['img_name'].tolist()\n",
    "        # Copy the selected files into the destination directory\n",
    "        for file in selected_files:\n",
    "            source_path = os.path.join(source_dir, file)\n",
    "            destination_path = os.path.join(destination_dir, file)\n",
    "            shutil.copy(source_path, destination_path)\n",
    "\n",
    "copy_random_samples(df_remaining_train_labels, train_dir_food251, train_dir_food101_unknown, n_samples_per_remaining_class_train, SEED)\n",
    "copy_random_samples(df_remaining_val_labels, val_dir_food251, test_dir_food101_unknown, n_samples_per_remaining_class_test, SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extented the Unknow Class with New Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download other typical food types to be added to the unknown category\n",
    "other_images = ['capuccino cup', 'coffee', 'banana', 'obst', 'apfel', 'orange fruit', 'fruit basket', 'smoothie', 'dorade', 'kabeljau']\n",
    "#for item in other_images:\n",
    "#    downloader.download(item, limit=25, output_dir='images', adult_filter_off=True, force_replace=False, timeout=60, filter=\"photo, clipart\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 6 images of capuccino cup to 'train' and 2 images to 'test'.\n",
      "Moved 6 images of coffee to 'train' and 2 images to 'test'.\n",
      "Moved 6 images of banana to 'train' and 2 images to 'test'.\n",
      "Moved 6 images of obst to 'train' and 2 images to 'test'.\n",
      "Moved 6 images of apfel to 'train' and 2 images to 'test'.\n",
      "Moved 6 images of orange fruit to 'train' and 2 images to 'test'.\n",
      "Moved 6 images of fruit basket to 'train' and 2 images to 'test'.\n",
      "Moved 6 images of smoothie to 'train' and 2 images to 'test'.\n",
      "Moved 6 images of dorade to 'train' and 2 images to 'test'.\n",
      "Moved 6 images of kabeljau to 'train' and 2 images to 'test'.\n"
     ]
    }
   ],
   "source": [
    "# Create \"train\" and \"test\" directories if they don't exist\n",
    "other_image_folder = Path('images')\n",
    "other_image_folder_train = other_image_folder / 'train'\n",
    "other_image_folder_test = other_image_folder / 'test'\n",
    "other_image_folder_train.mkdir(parents=True, exist_ok=True)\n",
    "other_image_folder_test.mkdir(parents=True, exist_ok=True)  \n",
    "\n",
    "# Loop through each category in \"other_images\"\n",
    "for category in other_images:\n",
    "    # List the files for this category (assumes the images are named according to the category)\n",
    "    category_folder = os.path.join(other_image_folder, category)\n",
    "    \n",
    "    # Check if the category folder exists\n",
    "    if os.path.exists(category_folder):\n",
    "        # Get all image filenames for this category\n",
    "        images_orig = [img for img in os.listdir(category_folder) if img.lower().endswith(('.jpg', '.jpeg'))]\n",
    "        images_renamed = [image.replace('.',f'_{category}.') for image in images_orig]\n",
    "        images_renamed = [image.replace(' ','_') for image in images_renamed]\n",
    "\n",
    "        # Shuffle the image filenames randomly\n",
    "        random.seed(SEED+random.randint(1, 1000))\n",
    "        random.shuffle(images_orig)\n",
    "        random.shuffle(images_renamed)\n",
    "\n",
    "\n",
    "        # Move 6 images to the \"train\" folder\n",
    "        for idx, img in enumerate(images_orig[:6]):\n",
    "            src_orig = os.path.join(category_folder, img)\n",
    "            dst_orig = os.path.join(train_dir_food101_unknown, images_renamed[idx])\n",
    "            dst_renamed = os.path.join(train_dir_food101_unknown, images_renamed[idx])\n",
    "            shutil.copy(src_orig, dst_orig)\n",
    "            shutil.move(dst_orig, dst_renamed)\n",
    "\n",
    "        # Move 2 images to the \"test\" folder\n",
    "        for idx, img in enumerate(images_orig[6:8]):\n",
    "            src_orig = os.path.join(category_folder, img)\n",
    "            dst_orig = os.path.join(test_dir_food101_unknown, images_renamed[idx])\n",
    "            dst_renamed = os.path.join(test_dir_food101_unknown, images_renamed[idx])\n",
    "            shutil.copy(src_orig, dst_orig)\n",
    "            shutil.move(dst_orig, dst_renamed)\n",
    "\n",
    "        print(f\"Moved 6 images of {category} to 'train' and 2 images to 'test'.\")\n",
    "    else:\n",
    "        print(f\"Category folder '{category}' not found. Skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 45 images to ..\\data\\food-101_100_percent_unknown_2\\train\\unknown\n",
      "Copied 8 images to ..\\data\\food-101_100_percent_unknown_2\\test\\unknown\n"
     ]
    }
   ],
   "source": [
    "# Paths to your source and destination directories\n",
    "source_dir_train = '../data/ADEChallengeData2016/images/training'\n",
    "source_dir_test = '../data/ADEChallengeData2016/images/validation'\n",
    "destination_dir = 'path/to/destination/directory'\n",
    "\n",
    "# Get a list of all files in the source directory\n",
    "all_files_train = [f for f in os.listdir(source_dir_train) if os.path.isfile(os.path.join(source_dir_train, f))]\n",
    "all_files_test = [f for f in os.listdir(source_dir_test) if os.path.isfile(os.path.join(source_dir_test, f))]\n",
    "\n",
    "# Filter the list to include only image files (you can adjust the extensions based on your images)\n",
    "image_files_train = [f for f in all_files_train if f.lower().endswith(('.jpg', '.jpeg'))]\n",
    "image_files_test = [f for f in all_files_test if f.lower().endswith(('.jpg', '.jpeg'))]\n",
    "\n",
    "# Randomly select 50 images\n",
    "random.seed(SEED+random.randint(1, 1000))\n",
    "selected_images_train = random.sample(image_files_train, 45)\n",
    "selected_images_test = random.sample(image_files_test, 8)\n",
    "\n",
    "# Copy each selected image to the destination directory\n",
    "for image in selected_images_train:\n",
    "    shutil.copy(os.path.join(source_dir_train, image), train_dir_food101_unknown)\n",
    "\n",
    "print(f\"Copied {len(selected_images_train)} images to {train_dir_food101_unknown}\")\n",
    "\n",
    "# Copy each selected image to the destination directory\n",
    "for image in selected_images_test:\n",
    "    shutil.copy(os.path.join(source_dir_test, image), test_dir_food101_unknown)\n",
    "\n",
    "print(f\"Copied {len(selected_images_test)} images to {test_dir_food101_unknown}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
